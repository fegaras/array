# SAC: Scalable Array Comprehensions

An array comprehension is a monolithic array construction
that is as expressive as basic SQL
by supporting a group-by syntax that allows us to capture many array computations in declarative form.

SAC translates array comprehensions to Scala code that calls Spark RDD
operations whose functional arguments call the Scala's Parallel
Collections library for multicore parallelism.

# Benchmarks

The SAC benchmarks were evaluated on [SDSC Comet](https://portal.xsede.org/sdsc-comet).
The SBATCH shell script used to run the benchmarks on Comet is in tests/spark file comet.run.
The log files generated by the scripts that contain the run times are run*.log in the same directory.

The cluster should support Slurm Workload Manager, Hadoop 2.*, and myhadoop.

You compile SAC, use `mvn install` on the top directory.

Steps to run the scripts on Comet (or on any Slurm-managed cluster):

1. Install [Scala 2.12](https://downloads.lightbend.com/scala/2.12.12/scala-2.12.12.tgz).
2. Install [Spark 3.0 on Hadoop 2.7](https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz).
3. Change SCALA_HOME and SPARK_HOME in the SBATCH scripts to point to your installations.
4. Execute the scripts using sbatch, eg, `sbatch comet.run`.
